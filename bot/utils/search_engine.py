import os
import json
import logging
import re
import requests
import urllib.parse
from typing import List, Dict, Any, Tuple, Set
from difflib import SequenceMatcher
from dotenv import load_dotenv

load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞
YANDEX_DISK_TOKEN = os.getenv("YANDEX_DISK_TOKEN")
YANDEX_DISK_FOLDER_PATH = os.getenv("YANDEX_DISK_FOLDER_PATH", "/")
DOCS_PUBLIC_KEY = os.getenv("DOCS_PUBLIC_KEY")

if not YANDEX_DISK_TOKEN:
    raise ValueError("‚ùå YANDEX_DISK_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
if not DOCS_PUBLIC_KEY:
    raise ValueError("‚ùå DOCS_PUBLIC_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")

BASE_URL = "https://cloud-api.yandex.net/v1/disk"
HEADERS = {
    "Authorization": f"OAuth {YANDEX_DISK_TOKEN}"
}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
SYNONYMS = {
    "–∫–∞–±–µ–ª—å": "cable", "–∫–Ω–∏–∫—Å": "knx", "–∫–Ω—Ö": "knx", "–¥–∞—Ç—á–∏–∫": "sensor",
    "—Ä–µ–ª–µ": "relay", "–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä": "controller", "–ø–∞–Ω–µ–ª—å": "panel",
    "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è": "manual", "–ø–∞—Å–ø–æ—Ä—Ç": "datasheet", "—É—Ä—Ä–∏": "urri",
    "—é—Ä–∏–∏": "urri", "—Ö–¥–ª": "hdl", "–±–∞—Å–ø—Ä–æ": "buspro", "–±–∞—Å–ø—Ä": "buspro",
    "–º–∞—Ç–µ–∫": "matech", "–º–∞—Ç–µ—á": "matech", "–π–∏–ª–∞–π—Ç": "yeelight",
    "–∏–∑–∏–∫—É–ª": "easycool", "–∫–∞–±–µ–ª": "cable", "–∑–∞–º–æ–∫": "lock",
    "–¥–≤–µ—Ä–Ω–æ–π": "door", "–∏–æ—Ç": "iot", "–∞–π–æ—Ç–∏": "iot", "—Ç–µ—Ö–Ω–∏—á–∫–∞": "technical"
}

def normalize_with_synonyms(query: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    query_lower = query.lower().strip()
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ –¥–ª—è –∑–∞–º–µ–Ω—ã –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–µ—Ä–≤—ã–º–∏
    for wrong, correct in sorted(SYNONYMS.items(), key=lambda x: -len(x[0])):
        query_lower = query_lower.replace(wrong, correct)
    cleaned = re.sub(r"[^a-z0-9\s]", " ", query_lower)
    return re.sub(r"\s+", " ", cleaned).strip()

def get_folder_contents(path: str) -> List[Dict]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–ø–∫–∏ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞"""
    url = f"{BASE_URL}/resources"
    params = {"path": path, "limit": 1000}
    response = requests.get(url, headers=HEADERS, params=params)
    response.raise_for_status()
    data = response.json()
    return data.get("_embedded", {}).get("items", [])

def build_docs_url(file_path: str) -> str:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ URL –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    base = YANDEX_DISK_FOLDER_PATH.rstrip("/")
    if file_path.startswith(base):
        relative_path = file_path[len(base):].lstrip("/")
    else:
        relative_path = file_path.lstrip("/")
    
    encoded_docs_key = urllib.parse.quote(DOCS_PUBLIC_KEY, safe="")
    encoded_path = urllib.parse.quote(relative_path, safe="/")
    filename = os.path.basename(file_path)
    encoded_name = urllib.parse.quote(filename, safe="")
    
    return (
        f"https://docs.360.yandex.ru/docs/view?"
        f"url=ya-disk-public%3A%2F%2F{encoded_docs_key}%3A%2F{encoded_path}"
        f"&name={encoded_name}&nosw=1"
    )

class SearchEngine:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    def __init__(self, index_file: str = "data/cache/file_index.json"):
        self.index_file = index_file
        self.file_index = self.load_index()
        
        # –ö—ç—à –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self._normalize_cache = {}
        
        # –°–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
        self.synonyms = {
            "–∫–∞–±–µ–ª—å": ["cable", "–ø—Ä–æ–≤–æ–¥", "wire", "–∫–∞–±–µ–ª"],
            "knx": ["–∫–Ω–∏–∫—Å", "–∫–Ω—Ö", "knx"],
            "–¥–∞—Ç—á–∏–∫": ["sensor", "—Å–µ–Ω—Å–æ—Ä", "–¥–µ—Ç–µ–∫—Ç–æ—Ä"],
            "—Ä–µ–ª–µ": ["relay", "—Ä–µ–ª", "–ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å"],
            "–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä": ["controller", "control", "—É–ø—Ä–∞–≤–ª—è—é—â–∏–π"],
            "–ø–∞–Ω–µ–ª—å": ["panel", "–ø–∞–Ω–µ–ª"],
            "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è": ["manual", "instruction", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"],
            "–ø–∞—Å–ø–æ—Ä—Ç": ["datasheet", "technical", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π"],
            "urri": ["—É—Ä—Ä–∏", "—é—Ä–∏–∏"],
            "hdl": ["—Ö–¥–ª"],
            "buspro": ["–±–∞—Å–ø—Ä–æ", "–±–∞—Å–ø—Ä"],
            "matech": ["–º–∞—Ç–µ–∫", "–º–∞—Ç–µ—á"],
            "yeelight": ["–π–∏–ª–∞–π—Ç", "yee light"],
            "easycool": ["–∏–∑–∏–∫—É–ª", "easy cool", "–∏–∑–∏ –∫—É–ª"],
            "–∫–∞–±–µ–ª": ["–∫–∞–±–µ–ª—å", "cable"],
            "–∑–∞–º–æ–∫": ["lock", "–¥–≤–µ—Ä–Ω–æ–π –∑–∞–º–æ–∫", "door lock"],
            "–∑–∞–º–∫–∏": ["locks", "–¥–≤–µ—Ä–Ω—ã–µ –∑–∞–º–∫–∏", "door locks"],
            "–¥–≤–µ—Ä–Ω–æ–π": ["door", "–¥–≤–µ—Ä–Ω–æ–π"],
            "iot": ["–∏–æ—Ç", "iot systems", "–∞–π–æ—Ç–∏"],
            "—Ç–µ—Ö–Ω–∏—á–∫–∞": ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è", "technical", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–ø–∞—Å–ø–æ—Ä—Ç"]
        }

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–∞–ø–∫–∏ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞
        self.folder_links = {
            "–∫–∞–±–µ–ª—å": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/02.%20iOT%20%D0%9A%D0%B0%D0%B1%D0%B5%D0%BB%D1%8C",
            "–∫–∞–±–µ–ª–∏": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/02.%20iOT%20%D0%9A%D0%B0%D0%B1%D0%B5%D0%BB%D1%8C",
            "cable": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/02.%20iOT%20%D0%9A%D0%B0%D0%B1%D0%B5%D0%BB%D1%8C",
            "—Ç–µ—Ö–Ω–∏—á–∫–∞ –Ω–∞ –∫–∞–±–µ–ª—å": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/02.%20iOT%20%D0%9A%D0%B0%D0%B1%D0%B5%D0%BB%D1%8C",
            "–∫–∞–±–µ–ª—å –∏–æ—Ç": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/02.%20iOT%20%D0%9A%D0%B0%D0%B1%D0%B5%D0%BB%D1%8C",
            "–∫–∞–±–µ–ª—å iot": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/02.%20iOT%20%D0%9A%D0%B0%D0%B1%D0%B5%D0%BB%D1%8C",
            "iot –∫–∞–±–µ–ª—å": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/02.%20iOT%20%D0%9A%D0%B0%D0%B1%D0%B5%D0%BB%D1%8C",
            
            "–∑–∞–º–æ–∫": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            "–∑–∞–º–∫–∏": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            "–¥–≤–µ—Ä–Ω–æ–π –∑–∞–º–æ–∫": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            "–¥–≤–µ—Ä–Ω—ã–µ –∑–∞–º–∫–∏": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            "–∑–∞–º–∫–∏ iot": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            "–∑–∞–º–∫–∏ –∏–æ—Ç": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            "iot –∑–∞–º–æ–∫": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            "door lock": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/01.%20iOT%20Systems/04.%20%D0%94%D0%B2%D0%B5%D1%80%D0%BD%D1%8B%D0%B5%20%D0%B7%D0%B0%D0%BC%D0%BA%D0%B8%20iOT%20Systems",
            
            "–∞–ª–∏—Å–∞": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/02.%20HDL/09.%20%D0%98%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%20%D0%B3%D0%BE%D0%BB%D0%BE%D1%81%D0%BE%D0%B2%D1%8B%D0%BC%D0%B8%20%D0%B0%D1%81%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BD%D1%82%D0%B0%D0%BC%D0%B8.%20Buspro%20%D0%B8%20KNX",
            "—è–Ω–¥–µ–∫—Å –∞–ª–∏—Å–∞": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/02.%20HDL/09.%20%D0%98%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%20%D0%B3%D0%BE%D0%BB%D0%BE%D1%81%D0%BE%D0%B2%D1%8B%D0%BC%D0%B8%20%D0%B0%D1%81%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BD%D1%82%D0%B0%D0%BC%D0%B8.%20Buspro%20%D0%B8%20KNX",
            "–≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/02.%20HDL/09.%20%D0%98%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%20%D0%B3%D0%BE%D0%BB%D0%BE%D1%81%D0%BE%D0%B2%D1%8B%D0%BC%D0%B8%20%D0%B0%D1%81%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BD%D1%82%D0%B0%D0%BC%D0%B8.%20Buspro%20%D0%B8%20KNX",
            "mgwip": "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/02.%20HDL/09.%20%D0%98%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%20%D0%B3%D0%BE%D0%BB%D0%BE%D1%81%D0%BE%D0%B2%D1%8B%D0%BC%D0%B8%20%D0%B0%D1%81%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BD%D1%82%D0%B0%D0%BC%D0%B8.%20Buspro%20%D0%B8%20KNX",
        }

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        self._alisa_keywords = {"–∞–ª–∏—Å", "—è–Ω–¥–µ–∫—Å –∞–ª–∏—Å", "yandex alice", "alisa", "mgwip", "–≥–æ–ª–æ—Å–æ–≤", "–≥–æ–ª–æ—Å–æ–≤–æ–π", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"}
        self._integration_keywords = {"–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏", "–Ω–∞—Å—Ç—Ä–æ–∏", "–ø–æ–¥–∫–ª—é—á–∏", "—Å–≤—è–∑–∫", "—Å–≤—è–∑–∞—Ç—å", "–æ–±—ä–µ–¥–∏–Ω–∏—Ç—å"}
        self._cable_keywords = {"–∫–∞–±–µ–ª—å", "cable", "—Ç–µ—Ö–Ω–∏—á–∫–∞ –Ω–∞ –∫–∞–±–µ–ª—å", "–∫–∞–±–µ–ª—å –∏–æ—Ç", "–∫–∞–±–µ–ª—å iot", "iot –∫–∞–±–µ–ª—å"}
        self._lock_keywords = {"–∑–∞–º–æ–∫", "–∑–∞–º–∫–∏", "–¥–≤–µ—Ä–Ω–æ–π –∑–∞–º–æ–∫", "–¥–≤–µ—Ä–Ω—ã–µ –∑–∞–º–∫–∏", "–∑–∞–º–∫–∏ iot", "–∑–∞–º–∫–∏ –∏–æ—Ç", "iot –∑–∞–º–æ–∫"}

    def load_index(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.index_file):
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
                    if isinstance(data, dict):
                        return list(data.values())
                    return data
            else:
                logging.warning(f"Index file {self.index_file} not found")
                return []
        except Exception as e:
            logging.error(f"Error loading index: {e}")
            return []

    def normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not text:
            return ""
        
        if text in self._normalize_cache:
            return self._normalize_cache[text]
        
        normalized = normalize_with_synonyms(text)
        self._normalize_cache[text] = normalized
        return normalized

    def expand_synonyms(self, query: str) -> List[str]:
        """–†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        normalized_query = self.normalize_text(query)
        words = normalized_query.split()
        
        if not words:
            return [normalized_query]
        
        expanded_queries = {normalized_query}  # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        for i, word in enumerate(words):
            if word in self.synonyms:
                for synonym in self.synonyms[word]:
                    new_words = words.copy()
                    new_words[i] = synonym
                    expanded_queries.add(' '.join(new_words))
        
        return list(expanded_queries)

    def should_redirect_to_folder(self, query: str) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–∏—Ç—å –Ω–∞ –ø–∞–ø–∫—É –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞"""
        query_lower = query.lower().strip()
        
        # –ò–°–ö–õ–Æ–ß–ê–ï–ú –∑–∞–ø—Ä–æ—Å—ã —Å KNX - –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –∏–¥—Ç–∏ –∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º—É –ø—Ä–∞–≤–∏–ª—É
        if "knx" in query_lower:
            return False, ""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        if query_lower in self.folder_links:
            return True, self.folder_links[query_lower]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–±–µ–ª–µ–π (–±–µ–∑ KNX)
        if any(keyword in query_lower for keyword in self._cable_keywords) and "knx" not in query_lower:
            return True, self.folder_links["–∫–∞–±–µ–ª—å"]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–º–∫–æ–≤
        if any(keyword in query_lower for keyword in self._lock_keywords):
            return True, self.folder_links["–∑–∞–º–∫–∏"]
        
        return False, ""

    def is_knx_cable_query(self, query: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ–∏—Å–∫–æ–º –∫–∞–±–µ–ª—è KNX"""
        query_lower = query.lower().strip()
        
        # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞–±–µ–ª—è KNX
        exact_knx_queries = {
            "–∫–∞–±–µ–ª—å knx", "knx –∫–∞–±–µ–ª—å", "cable knx", "knx cable",
            "knx –∫–∞–±–µ–ª", "ye00820", "j-y(st)y", "2x2x0,8"
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        if any(exact_query in query_lower for exact_query in exact_knx_queries):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        words = set(query_lower.split())
        has_knx = any("knx" in word for word in words)
        has_cable = any(word in {"–∫–∞–±–µ–ª—å", "cable", "–∫–∞–±–µ–ª"} for word in words)
        
        return has_knx and has_cable

    def is_alisa_integration_query(self, query: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å –Ø–Ω–¥–µ–∫—Å –ê–ª–∏—Å–æ–π"""
        query_lower = query.lower()
        
        has_alisa = any(keyword in query_lower for keyword in self._alisa_keywords)
        has_integration = any(keyword in query_lower for keyword in self._integration_keywords)
        
        return has_alisa and has_integration

    def get_alisa_integration_link(self) -> str:
        """–°—Å—ã–ª–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –≥–æ–ª–æ—Å–æ–≤—ã–º–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º–∏"""
        return "https://disk.360.yandex.ru/d/xJi6eEXBTq01sw/02.%20HDL/09.%20%D0%98%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%20%D0%B3%D0%BE%D0%BB%D0%BE%D1%81%D0%BE%D0%B2%D1%8B%D0%BC%D0%B8%20%D0%B0%D1%81%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BD%D1%82%D0%B0%D0%BC%D0%B8.%20Buspro%20%D0%B8%20KNX"

    def find_knx_cable_files(self) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—â–µ—Ç —Ñ–∞–π–ª—ã –∫–∞–±–µ–ª—è KNX"""
        knx_cable_keywords = [
            "ye00820", "j-y(st)y", "2x2x0,8", "knx –∫–∞–±–µ–ª—å", "–∫–∞–±–µ–ª—å knx", "–∫–∞–±–µ–ª—å j-y", "j-y st y"
        ]
        
        scored_files = []
        
        for file_data in self.file_index:
            file_name = file_data.get("name", "").lower()
            file_path = file_data.get("path", "").lower()
            search_text = f"{file_name} {file_path}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            score = 0
            for i, keyword in enumerate(knx_cable_keywords):
                if keyword in search_text:
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ –ø–æ—Ä—è–¥–∫—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    score += (len(knx_cable_keywords) - i) * 100
            
            if score > 0:
                scored_files.append((score, file_data))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scored_files.sort(key=lambda x: x[0], reverse=True)
        return [file_data for score, file_data in scored_files]

    def calculate_relevance(self, file_data: Dict[str, Any], query_variants: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞ –∑–∞–ø—Ä–æ—Å—É"""
        max_score = 0
        
        file_name = file_data.get("name", "").lower()
        file_path = file_data.get("path", "").lower()
        norm_name = file_data.get("norm_name", "").lower()
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_text = f"{file_name} {file_path} {norm_name}"
        
        for query in query_variants:
            score = 0
            query_words = set(query.split())
            
            # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            if query in file_name:
                score += 10
                
            # 2. –í—Å–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
            file_name_words = set(file_name.split())
            if query_words.issubset(file_name_words):
                score += 8
                
            # 3. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –∏–º–µ–Ω–∏
            if query in norm_name:
                score += 7
                
            # 4. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –ø—É—Ç–∏
            if query in file_path:
                score += 6
                
            # 5. –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å—Ç—Ä–æ–∫)
            name_similarity = SequenceMatcher(None, query, file_name).ratio()
            path_similarity = SequenceMatcher(None, query, file_path).ratio()
            score += max(name_similarity, path_similarity) * 5
            
            # 6. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
            for word in query_words:
                if word in file_name:
                    score += 2
                if word in norm_name:
                    score += 3
                if word in file_path:
                    score += 1
            
            # 7. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–∞–±–µ–ª–µ–π KNX
            if "–∫–∞–±–µ–ª—å" in query and "knx" in query:
                knx_bonus_keywords = {
                    "ye00820": 100,
                    "j-y(st)y": 80, 
                    "2x2x0,8": 60,
                    "knx –∫–∞–±–µ–ª—å": 40,
                    "–∫–∞–±–µ–ª—å knx": 40
                }
                
                for keyword, bonus in knx_bonus_keywords.items():
                    if keyword in search_text:
                        score += bonus
                
                if "–¥–∞—Ç—á–∏–∫" in search_text:
                    score -= 50
            
            max_score = max(max_score, score)
        
        return max_score

    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not query or not self.file_index:
            return []
        
        logging.info(f"üîç –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: '{query}'")
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        query_variants = self.expand_synonyms(query)
        logging.info(f"üìã –í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞: {query_variants}")
        
        scored_results = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        for file_data in self.file_index:
            relevance = self.calculate_relevance(file_data, query_variants)
            
            if relevance > 0:
                scored_results.append({
                    **file_data,
                    "relevance": relevance
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scored_results.sort(key=lambda x: x["relevance"], reverse=True)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        for i, result in enumerate(scored_results[:5]):
            logging.info(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}: {result['name']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['relevance']:.2f})")
        
        return scored_results[:limit]

    def hybrid_search(self, query: str, limit: int = 3) -> List[Dict[str, Any]]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞,
        –∑–∞—Ç–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
        """
        # 1. –°–ü–ï–¶–ò–ê–õ–¨–ù–û–ï –ü–†–ê–í–ò–õ–û –î–õ–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –° –ê–õ–ò–°–û–ô - –í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢
        if self.is_alisa_integration_query(query):
            logging.info(f"üéØ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ê–õ–ò–°–ê: –∑–∞–ø—Ä–æ—Å '{query}' ‚Üí —Å—Å—ã–ª–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é")
            return [{
                "name": "üìÅ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –Ø–Ω–¥–µ–∫—Å –ê–ª–∏—Å–æ–π",
                "path": self.get_alisa_integration_link(),
                "is_folder_link": True,
                "folder_link": self.get_alisa_integration_link()
            }]

        # 2. –°–ü–ï–¶–ò–ê–õ–¨–ù–û–ï –ü–†–ê–í–ò–õ–û –î–õ–Ø –ö–ê–ë–ï–õ–Ø KNX
        if self.is_knx_cable_query(query):
            knx_cable_results = self.find_knx_cable_files()
            if knx_cable_results:
                logging.info(f"üéØ –ö–ê–ë–ï–õ–¨ KNX: –Ω–∞–π–¥–µ–Ω–æ {len(knx_cable_results)} —Ñ–∞–π–ª–æ–≤")
                return knx_cable_results[:limit]

        # 3. –ü–†–û–í–ï–†–ö–ê –ü–ï–†–ï–ù–ê–ü–†–ê–í–õ–ï–ù–ò–Ø –ù–ê –ü–ê–ü–ö–ò –Ø–ù–î–ï–ö–°.–î–ò–°–ö–ê
        should_redirect, folder_link = self.should_redirect_to_folder(query)
        if should_redirect:
            logging.info(f"üéØ –ü–ï–†–ï–ù–ê–ü–†–ê–í–õ–ï–ù–ò–ï: –∑–∞–ø—Ä–æ—Å '{query}' ‚Üí –ø–∞–ø–∫–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞")
            return [{
                "name": f"üìÅ –ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π: {query}",
                "path": folder_link,
                "is_folder_link": True,
                "folder_link": folder_link
            }]

        # 4. –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö
        improved_results = self.search(query, limit * 2)  # –ò—â–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        improved_results = self.filter_irrelevant_results(improved_results, query)
        if improved_results:
            logging.info(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(improved_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return improved_results[:limit]
        
        # 5. –°–¢–ê–†–´–ô –ü–û–ò–°–ö –î–õ–Ø –û–ë–†–ê–¢–ù–û–ô –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò
        logging.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º")
        try:
            old_results = self.old_smart_search(query)
            return old_results[:limit]
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å—Ç–∞—Ä–æ–º –ø–æ–∏—Å–∫–µ: {e}")
            return []

    def old_smart_search(self, query: str, limit: int = 3) -> List[Dict]:
        """
        –°—Ç–∞—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        """
        if not self.file_index:
            return []

        query_norm = self.normalize_text(query)
        logging.info(f"üîç –°—Ç–∞—Ä—ã–π –ø–æ–∏—Å–∫: '{query}' -> –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π: '{query_norm}'")
        
        if not query_norm:
            return []

        # –£—Ä–æ–≤–µ–Ω—å 1: –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        exact_results = self._old_search_exact_match(query_norm)
        if exact_results:
            logging.info(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 1: –ù–∞–π–¥–µ–Ω–æ {len(exact_results)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            return exact_results[:limit]

        # –£—Ä–æ–≤–µ–Ω—å 2: –ü–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        combo_results = self._old_search_keyword_combinations(query_norm)
        if combo_results:
            logging.info(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 2: –ù–∞–π–¥–µ–Ω–æ {len(combo_results)} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π")
            return combo_results[:limit]

        # –£—Ä–æ–≤–µ–Ω—å 3: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –≤–∞–∂–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        important_results = self._old_search_important_keywords(query_norm)
        if important_results:
            logging.info(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 3: –ù–∞–π–¥–µ–Ω–æ {len(important_results)} –ø–æ –≤–∞–∂–Ω—ã–º —Å–ª–æ–≤–∞–º")
            return important_results[:limit]

        return []

    def _old_search_exact_match(self, query_norm: str) -> List[Dict]:
        """–°—Ç–∞—Ä—ã–π —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫"""
        keywords = [word for word in query_norm.split() if len(word) >= 2]
        if not keywords:
            return []

        results = []
        for file_data in self.file_index:
            norm_name = file_data.get("norm_name", "")
            if all(kw in norm_name for kw in keywords):
                results.append(file_data)
        
        return results

    def _old_search_keyword_combinations(self, query_norm: str) -> List[Dict]:
        """–°—Ç–∞—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º"""
        keywords = [word for word in query_norm.split() if len(word) >= 2]
        if len(keywords) < 2:
            return []

        scored_files = []
        
        for file_data in self.file_index:
            norm_name = file_data.get("norm_name", "")
            file_name = file_data.get("name", "").lower()
            
            score = 0
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            matched_keywords = sum(1 for kw in keywords if kw in norm_name)
            if matched_keywords >= 2:
                score += matched_keywords * 20
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∏–º–µ–Ω–∏
            matched_in_original = sum(1 for kw in keywords if kw in file_name)
            score += matched_in_original * 10
            
            if score > 0:
                scored_files.append((score, file_data))
        
        scored_files.sort(key=lambda x: x[0], reverse=True)
        return [file_data for score, file_data in scored_files]

    def _old_search_important_keywords(self, query_norm: str) -> List[Dict]:
        """–°—Ç–∞—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤–∞–∂–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        important_keywords = {"alisa", "knx", "integration", "connect", "gateway", "voice"}
        
        # –ù–∞—Ö–æ–¥–∏–º –≤–∞–∂–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ
        found_important = [kw for kw in important_keywords if kw in query_norm]
        if not found_important:
            return []

        scored_files = []
        for file_data in self.file_index:
            norm_name = file_data.get("norm_name", "")
            
            score = sum(30 for keyword in found_important if keyword in norm_name)
            
            if score > 0:
                scored_files.append((score, file_data))
        
        scored_files.sort(key=lambda x: x[0], reverse=True)
        return [file_data for score, file_data in scored_files]


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
async def smart_document_search(query: str, limit: int = 3) -> List[Dict[str, Any]]:
    """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    search_engine = SearchEngine()
    return search_engine.hybrid_search(query, limit)

def search_in_file_index(query: str, index_path: str = "data/cache/file_index.json") -> List[Dict]:
    """–°—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
    search_engine = SearchEngine(index_path)
    return search_engine.search(query)

def has_only_technical_files(results: List[Dict[str, Any]]) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
    """
    # –ï—Å–ª–∏ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –ø–∞–ø–∫—É - –Ω–µ —Å—á–∏—Ç–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º
    if results and results[0].get("is_folder_link"):
        return False
        
    technical_patterns = ["r5-", "–¥–∞—Ç—á–∏–∫", "sensor", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫", "–ø–∞—Å–ø–æ—Ä—Ç", "technical"]
    
    for file_data in results:
        file_name = file_data.get("name", "").lower()
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å –æ–¥–∏–Ω –ù–ï —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ñ–∞–π–ª - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
        if not any(pattern in file_name for pattern in technical_patterns):
            return False
    
    # –í—Å–µ —Ñ–∞–π–ª—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ
    return True

def should_use_ai_directly(query: str) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ —Å—Ä–∞–∑—É –ø–æ–¥–∫–ª—é—á–∞—Ç—å –ò–ò –±–µ–∑ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
    """
    query_lower = query.lower().strip()
    
    # –ó–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ä–∞–∑—É –∏–¥—É—Ç –∫ –ò–ò (—Å–ª–æ–∂–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã)
    ai_direct_keywords = {
        # –û–±—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        "–∫–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å", "–∫–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å", "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å", "–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", "–∫–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å", "–∫–∞–∫ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å",
        "–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ç–æ–∫–æ–ª", "–ø–æ–¥–∫–ª—é—á–∏—Ç—å –ø—Ä–æ—Ç–æ–∫–æ–ª", "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Ç–æ–∫–æ–ª",
        "–ø—Ä–æ—Ç–æ–∫–æ–ª—ã —É–º–Ω–æ–≥–æ –¥–æ–º–∞", "—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤", 
        "–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤", "—Å–≤—è–∑—å –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤",
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –≤—ã–±–æ—Ä
        "–∫–∞–∫–æ–π –ª—É—á—à–µ", "—á—Ç–æ –≤—ã–±—Ä–∞—Ç—å", "—Å—Ä–∞–≤–Ω–∏—Ç–µ", "–æ—Ç–ª–∏—á–∏—è", "—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É",
        "–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞", "–Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏", "–ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã",
        
        # –ü—Ä–æ–±–ª–µ–º—ã –∏ –æ—à–∏–±–∫–∏
        "–ø—Ä–æ–±–ª–µ–º–∞ —Å", "–æ—à–∏–±–∫–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è", 
        "–Ω–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è", "—Å–ª–æ–º–∞–ª", "–Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç",
        
        # –û–±—ä—è—Å–Ω–µ–Ω–∏—è
        "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–∫–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω", "–ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã", "–æ–±—ä—è—Å–Ω–∏—Ç–µ",
        "—Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–≤ —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞",
        
        # –°–ª–æ–∂–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã
        "–ø—Ä–æ—Ç–æ–∫–æ–ª", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏", "api", "rest api", "websocket", "mqtt",
        "knx ip", "bacnet", "modbus", "zigbee", "z-wave", "wi-fi",
        "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏", "—Å—Ü–µ–Ω–∞—Ä–∏–π", "—Å—Ü–µ–Ω–∞—Ä", "–ª–æ–≥–∏–∫–∞"
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ò–ò
    has_ai_keywords = any(keyword in query_lower for keyword in ai_direct_keywords)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
    is_howto_question = any(phrase in query_lower for phrase in [
        "–∫–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å", "–∫–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å", "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç"
    ])
    
    is_protocol_question = any(word in query_lower for word in [
        "–ø—Ä–æ—Ç–æ–∫–æ–ª", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏", "api", "—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å", "–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ"
    ])
    
    is_complex_technical = any(word in query_lower for word in [
        "–ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã", "–æ–±—ä—è—Å–Ω–∏—Ç–µ", "—Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ", "—á—Ç–æ —Ç–∞–∫–æ–µ", "—Å—Ä–∞–≤–Ω–∏—Ç–µ"
    ])
    
    # –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
    if has_ai_keywords:
        logging.info(f"‚úÖ –†–µ—à–µ–Ω–∏–µ: —Å–ª–æ–∂–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å '{query}' ‚Üí –∫ –ò–ò")
        return True
    
    # –û—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ò–ò
    simple_search_keywords = {
        "–¥–æ–∫—É–º–µ–Ω—Ç", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", "–ø–∞—Å–ø–æ—Ä—Ç", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
        "—Å–∫–∞—á–∞—Ç—å", "—Ñ–∞–π–ª", "pdf", "—Å—Ö–µ–º–∞", "—á–µ—Ä—Ç–µ–∂", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫",
        "–∞–ª–∏—Å", "mgwip", "–∫–∞–±–µ–ª—å", "–∑–∞–º–æ–∫", "–¥–∞—Ç—á–∏–∫", "—Ä–µ–ª–µ"
    }
    
    if any(keyword in query_lower for keyword in simple_search_keywords):
        logging.info(f"‚ùå –†–µ—à–µ–Ω–∏–µ: –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ '{query}' ‚Üí –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫")
        return False
    
    # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ - –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
    if len(query_lower.split()) <= 2:
        logging.info(f"‚ùå –†–µ—à–µ–Ω–∏–µ: –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å '{query}' ‚Üí –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫")
        return False
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ò–ò
    words_count = len(query_lower.split())
    if words_count >= 4:  # –î–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        logging.info(f"‚úÖ –†–µ—à–µ–Ω–∏–µ: –¥–ª–∏–Ω–Ω—ã–π —Å–ª–æ–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å '{query}' ‚Üí –∫ –ò–ò")
        return True
    
    logging.info(f"‚ùå –†–µ—à–µ–Ω–∏–µ: –æ–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å '{query}' ‚Üí –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫")
    return False

def filter_irrelevant_results(self, results: List[Dict], query: str) -> List[Dict]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —è–≤–Ω–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    if not results:
        return []
    
    query_lower = query.lower()
    filtered_results = []
    
    for result in results:
        name = result.get('name', '').lower()
        path = result.get('path', '').lower()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Å–ø–æ—Ä—Ç–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if any(word in query_lower for word in ['–∏–Ω—Ç–µ–≥—Ä–∞—Ü', '–ø—Ä–æ—Ç–æ–∫–æ–ª', 'api', '–Ω–∞—Å—Ç—Ä–æ–π–∫']):
            if any(tech_word in name for tech_word in ['–ø–∞—Å–ø–æ—Ä—Ç', 'datasheet', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫', 'r5-']):
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Å–ø–æ—Ä—Ç–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        filtered_results.append(result)
    
    return filtered_results