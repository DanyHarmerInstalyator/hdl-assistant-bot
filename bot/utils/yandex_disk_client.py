# import os
# import re
# import requests
# import urllib.parse
# import json
# from typing import List, Dict
# from dotenv import load_dotenv
# from .synonyms import SYNONYMS


# def normalize_with_synonyms(query: str) -> str:
#     query_lower = query.lower().strip()
#     for wrong, correct in sorted(SYNONYMS.items(), key=lambda x: -len(x[0])):
#         query_lower = query_lower.replace(wrong, correct)
#     cleaned = re.sub(r"[^a-z0-9\s]", " ", query_lower)
#     return re.sub(r"\s+", "", cleaned)

# load_dotenv()

# YANDEX_DISK_TOKEN = os.getenv("YANDEX_DISK_TOKEN")
# YANDEX_DISK_FOLDER_PATH = os.getenv("YANDEX_DISK_FOLDER_PATH", "/")
# DOCS_PUBLIC_KEY = os.getenv("DOCS_PUBLIC_KEY")

# if not YANDEX_DISK_TOKEN:
#     raise ValueError("‚ùå YANDEX_DISK_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
# if not DOCS_PUBLIC_KEY:
#     raise ValueError("‚ùå DOCS_PUBLIC_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")


# BASE_URL = "https://cloud-api.yandex.net/v1/disk"

# HEADERS = {
#     "Authorization": f"OAuth {YANDEX_DISK_TOKEN}"
# }

# def normalize_text(s: str) -> str:
#     return re.sub(r"[^a-z0-9]", "", s.lower())

# def get_folder_contents(path: str) -> List[Dict]:
#     url = f"{BASE_URL}/resources"
#     params = {"path": path, "limit": 1000}
#     response = requests.get(url, headers=HEADERS, params=params)
#     response.raise_for_status()
#     data = response.json()
#     return data.get("_embedded", {}).get("items", [])

# def get_docs_public_key() -> str:
#     return DOCS_PUBLIC_KEY

# def build_docs_url(file_path: str) -> str:
#     docs_key = get_docs_public_key()
#     encoded_docs_key = urllib.parse.quote(docs_key, safe="")
#     base = YANDEX_DISK_FOLDER_PATH.rstrip("/")
#     if file_path.startswith(base):
#         relative_path = file_path[len(base):].lstrip("/")
#     else:
#         relative_path = file_path.lstrip("/")
#     encoded_path = urllib.parse.quote(relative_path, safe="/")
#     filename = os.path.basename(file_path)
#     encoded_name = urllib.parse.quote(filename, safe="")
#     return (
#         f"https://docs.360.yandex.ru/docs/view?"
#         f"url=ya-disk-public%3A%2F%2F{encoded_docs_key}%3A%2F{encoded_path}"
#         f"&name={encoded_name}&nosw=1"
#     )
# def search_in_file_index(query: str, index_path: str = "data/cache/file_index.json") -> List[Dict]:
#     if not os.path.exists(index_path):
#         return []
    
#     with open(index_path, "r", encoding="utf-8") as f:
#         files = json.load(f)
    
#     query_norm = normalize_with_synonyms(query)
#     if not query_norm:
#         return []

#     results = []
#     for f in files:
#         if query_norm in f["norm_name"] or f["norm_name"] in query_norm:
#             results.append(f)
#     return results


# bot/utils/yandex_disk_client.py QWEEN
# import os
# import re
# import requests
# import urllib.parse
# import json
# from typing import List, Dict
# from dotenv import load_dotenv
# from .synonyms import SYNONYMS
# from .ai_fallback import analyze_relevance

# def normalize_with_synonyms(query: str) -> str:
#     query_lower = query.lower().strip()
#     for wrong, correct in sorted(SYNONYMS.items(), key=lambda x: -len(x[0])):
#         query_lower = query_lower.replace(wrong, correct)
#     cleaned = re.sub(r"[^a-z0-9\s]", " ", query_lower)
#     return re.sub(r"\s+", " ", cleaned).strip()

# load_dotenv()

# YANDEX_DISK_TOKEN = os.getenv("YANDEX_DISK_TOKEN")
# YANDEX_DISK_FOLDER_PATH = os.getenv("YANDEX_DISK_FOLDER_PATH", "/")
# DOCS_PUBLIC_KEY = os.getenv("DOCS_PUBLIC_KEY")

# if not YANDEX_DISK_TOKEN:
#     raise ValueError("‚ùå YANDEX_DISK_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
# if not DOCS_PUBLIC_KEY:
#     raise ValueError("‚ùå DOCS_PUBLIC_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")

# BASE_URL = "https://cloud-api.yandex.net/v1/disk"
# HEADERS = {
#     "Authorization": f"OAuth {YANDEX_DISK_TOKEN}"
# }

# def get_folder_contents(path: str) -> List[Dict]:
#     url = f"{BASE_URL}/resources"
#     params = {"path": path, "limit": 1000}
#     response = requests.get(url, headers=HEADERS, params=params)
#     response.raise_for_status()
#     data = response.json()
#     return data.get("_embedded", {}).get("items", [])

# def get_docs_public_key() -> str:
#     return DOCS_PUBLIC_KEY

# def build_docs_url(file_path: str) -> str:
#     docs_key = get_docs_public_key()
#     encoded_docs_key = urllib.parse.quote(docs_key, safe="")
#     base = YANDEX_DISK_FOLDER_PATH.rstrip("/")
#     if file_path.startswith(base):
#         relative_path = file_path[len(base):].lstrip("/")
#     else:
#         relative_path = file_path.lstrip("/")
#     encoded_path = urllib.parse.quote(relative_path, safe="/")
#     filename = os.path.basename(file_path)
#     encoded_name = urllib.parse.quote(filename, safe="")
#     return (
#         f"https://docs.360.yandex.ru/docs/view?"
#         f"url=ya-disk-public%3A%2F%2F{encoded_docs_key}%3A%2F{encoded_path}"
#         f"&name={encoded_name}&nosw=1"
#     )

# async def search_with_ai_relevance(query: str, index_path: str = "data/cache/file_index.json", limit: int = 3) -> List[Dict]:
#     """
#     –ü–æ–∏—Å–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ò–ò
#     """
#     if not os.path.exists(index_path):
#         return []
    
#     with open(index_path, "r", encoding="utf-8") as f:
#         files = json.load(f)
    
#     query_norm = normalize_with_synonyms(query)
#     print(f"üîç –ó–∞–ø—Ä–æ—Å: '{query}' -> –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π: '{query_norm}'")
    
#     if not query_norm:
#         return []

#     # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –±–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ –ò–ò
#     basic_results = basic_search(query_norm, files, limit=15)
    
#     if not basic_results:
#         return []
    
#     # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–µ–Ω –ª–∏ –ò–ò –∞–Ω–∞–ª–∏–∑
#     needs_ai_analysis = needs_ai_relevance_check(query, query_norm, basic_results)
    
#     if not needs_ai_analysis:
#         print(f"‚úÖ –ò–ò –∞–Ω–∞–ª–∏–∑ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º {len(basic_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
#         return basic_results[:limit]
    
#     # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ò–ò –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
#     print(f"üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º –ò–ò –∞–Ω–∞–ª–∏–∑ –¥–ª—è {len(basic_results)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
#     relevant_files = []
    
#     for file_data in basic_results[:8]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
#         file_name = file_data.get("name", "")
#         is_relevant = await analyze_relevance(query, file_name)
#         if is_relevant:
#             relevant_files.append(file_data)
#         if len(relevant_files) >= limit:
#             break
    
#     print(f"‚úÖ –ü–æ—Å–ª–µ –ò–ò –∞–Ω–∞–ª–∏–∑–∞ –æ—Å—Ç–∞–ª–æ—Å—å {len(relevant_files)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
#     return relevant_files

# def needs_ai_relevance_check(original_query: str, normalized_query: str, results: List[Dict]) -> bool:
#     """
#     –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–µ–Ω –ª–∏ –∞–Ω–∞–ª–∏–∑ –ò–ò –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
#     """
#     # –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ –±—Ä–µ–Ω–¥–∞–º –Ω–µ —Ç—Ä–µ–±—É—é—Ç –ò–ò
#     simple_brands = ["urri", "hdl", "buspro", "matech", "yeelight", "dali", "granit"]
#     if normalized_query in simple_brands:
#         return False
    
#     # –ó–∞–ø—Ä–æ—Å—ã —Å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –ò–ò
#     if len(original_query.split()) <= 2:
#         return False
    
#     # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –ò–ò –∞–Ω–∞–ª–∏–∑–∞
#     complex_keywords = ["–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏", "–ø–æ–¥–∫–ª—é—á–∏", "–Ω–∞—Å—Ç—Ä–æ–∏", "–∞–ª–∏—Å", "–≥–æ–ª–æ—Å–æ–≤", "—É–ø—Ä–∞–≤–ª–µ–Ω"]
#     if any(keyword in original_query.lower() for keyword in complex_keywords):
#         return True
    
#     # –ï—Å–ª–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –µ—Å—Ç—å —Ñ–∞–π–ª—ã —Å "alisa" –∏ "knx" - –Ω—É–∂–µ–Ω –ò–ò –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
#     for file_data in results[:5]:
#         file_name = file_data.get("name", "").lower()
#         if "alisa" in file_name and "knx" in file_name:
#             return True
    
#     return False

# def basic_search(query_norm: str, files: List[Dict], limit: int = 15) -> List[Dict]:
#     """
#     –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –±–µ–∑ –ò–ò
#     """
#     scored_files = []
    
#     for file_data in files:
#         file_name = file_data.get("name", "").lower()
#         norm_name = file_data.get("norm_name", "")
        
#         score = 0
        
#         # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –∏–º–µ–Ω–∏
#         if query_norm in norm_name:
#             score += 100
        
#         # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∏–º–µ–Ω–∏
#         if query_norm in file_name:
#             score += 50
        
#         # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
#         words = query_norm.split()
#         if words:
#             matched_words = sum(1 for word in words if word in norm_name)
#             score += matched_words * 10
        
#         if score > 0:
#             scored_files.append((score, file_data))
    
#     scored_files.sort(key=lambda x: x[0], reverse=True)
#     return [file_data for score, file_data in scored_files[:limit]]

# def smart_search(query: str, index_path: str = "data/cache/file_index.json") -> List[Dict]:
#     import asyncio
#     return asyncio.run(search_with_ai_relevance(query, index_path))

# def search_in_file_index(query: str, index_path: str = "data/cache/file_index.json") -> List[Dict]:
#     """
#     –ò—â–µ—Ç —Ñ–∞–π–ª—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞.
#     –ü—Ä–∏–º–µ—Ä: "–ê–ª–∏—Å–∞ KNX" ‚Üí –∏—â–µ—Ç —Ñ–∞–π–ª—ã, –≥–¥–µ –µ—Å—Ç—å –∏ "alisa", –∏ "knx".
#     """
#     if not os.path.exists(index_path):
#         return []
    
#     with open(index_path, "r", encoding="utf-8") as f:
#         files = json.load(f)
    
#     query_norm = normalize_with_synonyms(query)
#     if not query_norm:
#         return []

#     # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–º–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞)
#     keywords = [word for word in re.split(r"[^a-z0-9]", query_norm) if len(word) >= 2]
    
#     if not keywords:
#         return []

#     results = []
#     for f in files:
#         norm_name = f["norm_name"]
#         # –¢—Ä–µ–±—É–µ–º, —á—Ç–æ–±—ã –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª–∏
#         if all(kw in norm_name for kw in keywords):
#             results.append(f)
    
#     return results

import os
import re
import requests
import urllib.parse
import json
from typing import List, Dict
from dotenv import load_dotenv
from .synonyms import SYNONYMS
from .ai_fallback import analyze_relevance

def normalize_with_synonyms(query: str) -> str:
    query_lower = query.lower().strip()
    for wrong, correct in sorted(SYNONYMS.items(), key=lambda x: -len(x[0])):
        query_lower = query_lower.replace(wrong, correct)
    cleaned = re.sub(r"[^a-z0-9\s]", " ", query_lower)
    return re.sub(r"\s+", " ", cleaned).strip()

load_dotenv()

YANDEX_DISK_TOKEN = os.getenv("YANDEX_DISK_TOKEN")
YANDEX_DISK_FOLDER_PATH = os.getenv("YANDEX_DISK_FOLDER_PATH", "/")
DOCS_PUBLIC_KEY = os.getenv("DOCS_PUBLIC_KEY")

if not YANDEX_DISK_TOKEN:
    raise ValueError("‚ùå YANDEX_DISK_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
if not DOCS_PUBLIC_KEY:
    raise ValueError("‚ùå DOCS_PUBLIC_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")

BASE_URL = "https://cloud-api.yandex.net/v1/disk"
HEADERS = {
    "Authorization": f"OAuth {YANDEX_DISK_TOKEN}"
}

def get_folder_contents(path: str) -> List[Dict]:
    url = f"{BASE_URL}/resources"
    params = {"path": path, "limit": 1000}
    response = requests.get(url, headers=HEADERS, params=params)
    response.raise_for_status()
    data = response.json()
    return data.get("_embedded", {}).get("items", [])

def get_docs_public_key() -> str:
    return DOCS_PUBLIC_KEY

def build_docs_url(file_path: str) -> str:
    docs_key = get_docs_public_key()
    encoded_docs_key = urllib.parse.quote(docs_key, safe="")
    base = YANDEX_DISK_FOLDER_PATH.rstrip("/")
    if file_path.startswith(base):
        relative_path = file_path[len(base):].lstrip("/")
    else:
        relative_path = file_path.lstrip("/")
    encoded_path = urllib.parse.quote(relative_path, safe="/")
    filename = os.path.basename(file_path)
    encoded_name = urllib.parse.quote(filename, safe="")
    return (
        f"https://docs.360.yandex.ru/docs/view?"
        f"url=ya-disk-public%3A%2F%2F{encoded_docs_key}%3A%2F{encoded_path}"
        f"&name={encoded_name}&nosw=1"
    )

async def smart_document_search(query: str, index_path: str = "data/cache/file_index.json") -> List[Dict]:
    """
    –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –ª–æ–≥–∏–∫–æ–π
    """
    if not os.path.exists(index_path):
        return []
    
    with open(index_path, "r", encoding="utf-8") as f:
        files = json.load(f)
    
    query_norm = normalize_with_synonyms(query)
    print(f"üîç –£–º–Ω—ã–π –ø–æ–∏—Å–∫: '{query}' -> –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π: '{query_norm}'")
    
    if not query_norm:
        return []

    # –£—Ä–æ–≤–µ–Ω—å 1: –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    exact_results = search_exact_match(query_norm, files)
    if exact_results:
        print(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 1: –ù–∞–π–¥–µ–Ω–æ {len(exact_results)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        return exact_results[:3]

    # –£—Ä–æ–≤–µ–Ω—å 2: –ü–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    combo_results = search_keyword_combinations(query_norm, files)
    if combo_results:
        print(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 2: –ù–∞–π–¥–µ–Ω–æ {len(combo_results)} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π")
        return combo_results[:3]

    # –£—Ä–æ–≤–µ–Ω—å 3: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –≤–∞–∂–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    important_results = search_important_keywords(query_norm, files)
    if important_results:
        print(f"‚úÖ –£—Ä–æ–≤–µ–Ω—å 3: –ù–∞–π–¥–µ–Ω–æ {len(important_results)} –ø–æ –≤–∞–∂–Ω—ã–º —Å–ª–æ–≤–∞–º")
        return important_results[:3]

    # –£—Ä–æ–≤–µ–Ω—å 4: –ê–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ò–ò (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
    if should_use_ai_analysis(query, query_norm):
        print(f"üîç –£—Ä–æ–≤–µ–Ω—å 4: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ò–ò –∞–Ω–∞–ª–∏–∑")
        ai_results = await search_with_ai_relevance(query, files)
        if ai_results:
            print(f"‚úÖ –ò–ò –∞–Ω–∞–ª–∏–∑: –Ω–∞–π–¥–µ–Ω–æ {len(ai_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            return ai_results[:3]

    return []

def search_exact_match(query_norm: str, files: List[Dict]) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    """
    keywords = [word for word in query_norm.split() if len(word) >= 2]
    if not keywords:
        return []

    results = []
    for file_data in files:
        norm_name = file_data.get("norm_name", "")
        if all(kw in norm_name for kw in keywords):
            results.append(file_data)
    
    return results

def search_keyword_combinations(query_norm: str, files: List[Dict]) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—Å–µ—Ö)
    """
    keywords = [word for word in query_norm.split() if len(word) >= 2]
    if len(keywords) < 2:
        return []

    scored_files = []
    
    for file_data in files:
        norm_name = file_data.get("norm_name", "")
        file_name = file_data.get("name", "").lower()
        
        score = 0
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if "alisa" in norm_name and "knx" in norm_name:
            score += 100  # –í—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ê–ª–∏—Å–∞ + KNX
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        matched_keywords = sum(1 for kw in keywords if kw in norm_name)
        if matched_keywords >= 2:  # –•–æ—Ç—è –±—ã 2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
            score += matched_keywords * 20
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∏–º–µ–Ω–∏
        matched_in_original = sum(1 for kw in keywords if kw in file_name)
        score += matched_in_original * 10
        
        if score > 0:
            scored_files.append((score, file_data))
    
    scored_files.sort(key=lambda x: x[0], reverse=True)
    return [file_data for score, file_data in scored_files]

def search_important_keywords(query_norm: str, files: List[Dict]) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –≤–∞–∂–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    """
    important_keywords = ["alisa", "knx", "integration", "connect", "gateway", "voice"]
    
    # –ù–∞—Ö–æ–¥–∏–º –≤–∞–∂–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ
    found_important = [kw for kw in important_keywords if kw in query_norm]
    if not found_important:
        return []

    scored_files = []
    for file_data in files:
        norm_name = file_data.get("norm_name", "")
        
        score = 0
        for keyword in found_important:
            if keyword in norm_name:
                score += 30  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        
        if score > 0:
            scored_files.append((score, file_data))
    
    scored_files.sort(key=lambda x: x[0], reverse=True)
    return [file_data for score, file_data in scored_files]

def should_use_ai_analysis(original_query: str, normalized_query: str) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ –∞–Ω–∞–ª–∏–∑ –ò–ò –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    """
    # –°–ª–æ–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    complex_patterns = [
        "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏", "–ø–æ–¥–∫–ª—é—á–∏", "–Ω–∞—Å—Ç—Ä–æ–∏", "—Å–≤—è–∑–∞—Ç—å", "–æ–±—ä–µ–¥–∏–Ω–∏—Ç—å",
        "–∞–ª–∏—Å", "–≥–æ–ª–æ—Å–æ–≤", "—É–ø—Ä–∞–≤–ª–µ–Ω", "–∫–∞–∫", "–∫–∞–∫–æ–π", "—á—Ç–æ –Ω—É–∂–Ω–æ"
    ]
    
    query_lower = original_query.lower()
    
    # –ó–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —Ç—Ä–µ–±—É—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    if any(pattern in query_lower for pattern in complex_patterns):
        return True
    
    # –ó–∞–ø—Ä–æ—Å—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏ (–ê–ª–∏—Å–∞ + KNX)
    if ("–∞–ª–∏—Å" in query_lower and "knx" in query_lower) or \
       ("–≥–æ–ª–æ—Å" in query_lower and "knx" in query_lower):
        return True
    
    return False

async def search_with_ai_relevance(query: str, files: List[Dict]) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ò–ò
    """
    # –ë–µ—Ä–µ–º —Ç–æ–ø-10 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    candidate_files = files[:10]
    
    relevant_files = []
    for file_data in candidate_files:
        file_name = file_data.get("name", "")
        is_relevant = await analyze_relevance(query, file_name)
        if is_relevant:
            relevant_files.append(file_data)
        if len(relevant_files) >= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            break
    
    return relevant_files

# –°—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
async def search_with_ai_relevance_old(query: str, index_path: str = "data/cache/file_index.json", limit: int = 3) -> List[Dict]:
    """–°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    if not os.path.exists(index_path):
        return []
    
    with open(index_path, "r", encoding="utf-8") as f:
        files = json.load(f)
    
    return await search_with_ai_relevance(query, files)

def search_in_file_index(query: str, index_path: str = "data/cache/file_index.json") -> List[Dict]:
    """
    –°—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    """
    if not os.path.exists(index_path):
        return []
    
    with open(index_path, "r", encoding="utf-8") as f:
        files = json.load(f)
    
    query_norm = normalize_with_synonyms(query)
    if not query_norm:
        return []

    return search_exact_match(query_norm, files)

def smart_search(query: str, index_path: str = "data/cache/file_index.json") -> List[Dict]:
    """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    import asyncio
    return asyncio.run(smart_document_search(query, index_path))